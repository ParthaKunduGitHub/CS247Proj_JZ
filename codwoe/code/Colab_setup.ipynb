{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthaKunduGitHub/CS247Proj_JZ/blob/main/codwoe/code/Colab_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxJOJu4XUIW"
      },
      "source": [
        "### Step 1: Mount the Google Drive or clone repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCSU4HrvkVDq",
        "outputId": "a487aa30-d4fa-4946-f024-927a100b0bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "from google.colab import userdata\n",
        "gh_pat = userdata.get('gh_pat')\n",
        "gh_username = userdata.get('gh_username')\n",
        "!rm -rf CS247Proj_JZ\n",
        "!git clone https://{gh_username}:{gh_pat}@github.com/ParthaKunduGitHub/CS247Proj_JZ.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGy1_W7kcg7U",
        "outputId": "e0377746-85d9-4e09-f255-f186f2454144"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS247Proj_JZ'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 137 (delta 49), reused 111 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (137/137), 1.84 MiB | 4.53 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Current Directory:\", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdwYSJibltw4",
        "outputId": "dc0add3d-cc30-4f61-cf83-494ace46ea66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd CS247Proj_JZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T98XkYHalx-_",
        "outputId": "f1148ec6-520d-4bef-ee6e-d138039f5d18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CS247Proj_JZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSNraHVJl3wC",
        "outputId": "c8ea53a5-7756-4411-ac17-3d8320d49b71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/CS247Proj_JZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z9FuihZQUWhd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyoSL1U8Xbjh"
      },
      "source": [
        "### Step 2: Open the project directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gfQ17SmkfOK",
        "outputId": "65cefc8a-6391-4cf7-ab76-2643bfac6f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/com_sci_247/codwoe\n"
          ]
        }
      ],
      "source": [
        "# cd /content/drive/MyDrive/com_sci_247/codwoe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTzYfAOEYN4C"
      },
      "source": [
        "### Step 3: Install required packages\n",
        "\n",
        "After installing them, Colab will require you to restart the session."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Rustup\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential cmake libhdf5-dev swig wget\n",
        "!curl https://sh.rustup.rs -sSf | sh -s -- -y\n",
        "!export PATH=\"$HOME/.cargo/bin:$PATH\""
      ],
      "metadata": {
        "id": "Dawk1VUebnNX",
        "outputId": "29931b22-9052-47cd-939d-7b688a9007be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libhdf5-dev is already the newest version (1.10.7+repack-4ubuntu2).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mIt looks like you have an existing rustup settings file at:\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0m/root/.rustup/settings.toml\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mRustup will install the default toolchain as specified in the settings file,\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0minstead of the one inferred from the default host triple.\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mUpdating existing toolchain, profile choice will be ignored\n",
            "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[0m\u001b[1mstable-x86_64-unknown-linux-gnu unchanged\u001b[0m - rustc 1.85.0 (4d91de4e4 2025-02-17)\n",
            "\n",
            "\u001b[0m\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[0m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, you need to source\n",
            "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
            "\n",
            "This is usually done by running one of the following (note the leading DOT):\n",
            ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
            "source \"$HOME/.cargo/env.fish\"  # For fish\n",
            "source \"$HOME/.cargo/env.nu\"    # For nushell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n"
      ],
      "metadata": {
        "id": "0IuIWG4CcHzJ",
        "outputId": "6ad8b8d3-29e3-4032-fa5c-48b1f024f361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.8.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFFKvhs4tAp5",
        "outputId": "8fd53759-95e6-44ff-93de-dd215793f580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.12.0 (from -r codwoe/requirements.txt (line 1))\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting cachetools==4.2.2 (from -r codwoe/requirements.txt (line 2))\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting certifi==2020.12.5 (from -r codwoe/requirements.txt (line 3))\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting chardet==4.0.0 (from -r codwoe/requirements.txt (line 4))\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting click==8.0.1 (from -r codwoe/requirements.txt (line 5))\n",
            "  Downloading click-8.0.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cycler==0.10.0 (from -r codwoe/requirements.txt (line 6))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting filelock==3.0.12 (from -r codwoe/requirements.txt (line 7))\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth==1.30.0 (from -r codwoe/requirements.txt (line 8))\n",
            "  Downloading google_auth-1.30.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting google-auth-oauthlib==0.4.4 (from -r codwoe/requirements.txt (line 9))\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting grpcio==1.37.1 (from -r codwoe/requirements.txt (line 10))\n",
            "  Downloading grpcio-1.37.1.tar.gz (21.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub==0.0.12 (from -r codwoe/requirements.txt (line 11))\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting idna==2.10 (from -r codwoe/requirements.txt (line 12))\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting joblib==1.0.1 (from -r codwoe/requirements.txt (line 13))\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting kiwisolver==1.3.1 (from -r codwoe/requirements.txt (line 14))\n",
            "  Downloading kiwisolver-1.3.1.tar.gz (53 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Markdown==3.3.4 (from -r codwoe/requirements.txt (line 15))\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting matplotlib==3.4.2 (from -r codwoe/requirements.txt (line 16))\n",
            "  Downloading matplotlib-3.4.2.tar.gz (37.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting moverscore==1.0.3 (from -r codwoe/requirements.txt (line 17))\n",
            "  Downloading moverscore-1.0.3.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nltk==3.6.7 (from -r codwoe/requirements.txt (line 18))\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from -r codwoe/requirements.txt (line 19)) (1.26.4)\n",
            "Collecting oauthlib==3.1.0 (from -r codwoe/requirements.txt (line 20))\n",
            "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting packaging==21.0 (from -r codwoe/requirements.txt (line 21))\n",
            "  Downloading packaging-21.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting Pillow==8.3.0 (from -r codwoe/requirements.txt (line 22))\n",
            "  Downloading Pillow-8.3.0.tar.gz (48.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker==2.3.0 (from -r codwoe/requirements.txt (line 23))\n",
            "  Downloading portalocker-2.3.0-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting protobuf==3.17.0 (from -r codwoe/requirements.txt (line 24))\n",
            "  Downloading protobuf-3.17.0-py2.py3-none-any.whl.metadata (858 bytes)\n",
            "Collecting pyasn1==0.4.8 (from -r codwoe/requirements.txt (line 25))\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyasn1-modules==0.2.8 (from -r codwoe/requirements.txt (line 26))\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pyemd (from -r codwoe/requirements.txt (line 27))\n",
            "  Downloading pyemd-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting pyparsing==2.4.7 (from -r codwoe/requirements.txt (line 28))\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting python-dateutil==2.8.1 (from -r codwoe/requirements.txt (line 29))\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting PyYAML==5.4.1 (from -r codwoe/requirements.txt (line 30))\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -r codwoe/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize==0.9.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyZTJENwrzwc",
        "outputId": "b147ad77-c4af-4357-f0b5-42357cb174dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize==0.9.0 in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize==0.9.0) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize==0.9.0) (25.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize==0.9.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize==0.9.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize==0.9.0) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize==0.9.0) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->scikit-optimize==0.9.0) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSoRzGXCfUtz"
      },
      "source": [
        "### Step 4: Train and Evaluate Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDPPdIwPajT5"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "66XqkuamajT5",
        "outputId": "fb197c9b-14ab-4c6b-de96-3112a3a570c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/CS247Proj_JZ\n",
            "2025-03-06 22:37:14.245604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741300634.276186   25064 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741300634.282619   25064 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-06 22:37:14.306125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CS247Proj_JZ/codwoe/code/revdict.py\", line 20, in <module>\n",
            "    import models\n",
            "ModuleNotFoundError: No module named 'models'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current Directory:\", os.getcwd())\n",
        "\n",
        "\n",
        "\n",
        "!python3 codwoe/code/revdict.py --do_train \\\n",
        "    --train_file data/en.train.json \\\n",
        "    --dev_file data/en.dev.json \\\n",
        "    --device cuda \\\n",
        "    --target_arch sgns \\\n",
        "    --summary_logdir /logs/revdict-baseline \\\n",
        "    --save_dir /models/revdict-baseline \\\n",
        "    --spm_model_path /models/revdict-baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC-8z2UxajT6"
      },
      "source": [
        "#### Testing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNynTEdrajT6",
        "outputId": "05afa799-b2c8-40fb-a000-36c3fb088825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-04 23:32:39.048254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741131159.068632   14339 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741131159.075363   14339 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-04 23:32:39.097018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-04 23:32:41,964 [DEBUG] revdict.py: Performing revdict prediction\n",
            "/content/drive/MyDrive/codwoe/baseline_archs/code/models.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file)\n",
            "/content/drive/MyDrive/codwoe/baseline_archs/code/data.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file)\n",
            "Pred.: 100% 6208/6208 [00:00<00:00, 7988.61it/s]\n"
          ]
        }
      ],
      "source": [
        "!python3 baseline_archs/code/revdict.py --do_pred \\\n",
        "    --test_file data/en.test.revdict.json \\\n",
        "    --device cuda \\\n",
        "    --target_arch sgns \\\n",
        "    --save_dir baseline_archs/models/revdict-baseline/sgns \\\n",
        "    --pred_file baseline_archs/models/revdict-baseline/sgns/revdict_predictions_sgns.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n84M6KLmkp2i",
        "outputId": "acd00a15-c919-4057-a3e5-8d2598f37755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-04 23:34:27.556998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741131267.577504   14927 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741131267.583653   14927 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-04 23:34:27.604048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
            "2025-03-04 23:34:38,504 [DEBUG] check_output.py: File \"baseline_archs/models/revdict-baseline/sgns/revdict_predictions_sgns.json\": no problems were identified.\n",
            "The submission will be understood as follows:\n",
            "\tSubmission on track revdict for language en, 6208 predictions.\n",
            "\tSubmission predicts these embeddings: sgns.\n"
          ]
        }
      ],
      "source": [
        "!python3 code/score.py \\\n",
        "    baseline_archs/models/revdict-baseline/sgns/revdict_predictions_sgns.json \\\n",
        "    --reference_files_dir data \\\n",
        "    --output_file baseline_archs/models/revdict-baseline/sgns/scores.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V2yunZ6Rofv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}